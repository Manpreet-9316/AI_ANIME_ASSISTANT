# ğŸ€ AI Anime Assistant - Backend

<div align="center">

[![Offline](https://img.shields.io/badge/ğŸ”’-100%25%20Offline-brightgreen?style=for-the-badge)](https://github.com)
[![Privacy](https://img.shields.io/badge/ğŸ›¡ï¸-Privacy%20First-blue?style=for-the-badge)](https://github.com)
[![No APIs](https://img.shields.io/badge/âœ…-Zero%20APIs-success?style=for-the-badge)](https://github.com)
[![License](https://img.shields.io/badge/ğŸ“„-MIT-yellow?style=for-the-badge)](https://github.com)

An advanced **fully offline** voice-enabled AI assistant backend with emotional intelligence, real-time speech processing, and intelligent action handling. This system powers an anime-style conversational AI that understands and responds to user emotions through natural language and voice.

ğŸ”’ **100% Offline & Privacy-Focused** - No cloud APIs, no external services, no data collection. All processing happens locally on your machine.

[Features](#-features) â€¢ [Setup](#-configuration) â€¢ [Usage](#-usage) â€¢ [Architecture](#-architecture) â€¢ [Offline](#-offline--privacy-features)

</div>

---

---

## ğŸ¯ Features

<table>
<tr>
<td>
  
ğŸ”’ **Fully Offline**
- No external APIs
- No cloud dependency
- Complete privacy

</td>
<td>

ğŸ™ï¸ **Voice I/O**
- Real-time capture
- Emotional synthesis
- Local processing

</td>
<td>

ğŸ˜Š **Emotion Detection**
- AI-powered analysis
- Transformer models
- Fallback detection

</td>
</tr>
<tr>
<td>

ğŸ§  **Local LLM**
- Ollama integration
- Runs on localhost
- No API keys

</td>
<td>

ğŸµ **Voice Cloning**
- On-device synthesis
- XTTS model
- Emotion-aware

</td>
<td>

ğŸ’¾ **Memory**
- Local storage
- Context-aware
- Multi-turn dialogue

</td>
</tr>
<tr>
<td>

âš¡ **Action Control**
- System actions
- Permission guards
- Safe defaults

</td>
<td>

ğŸ” **Intent Recognition**
- Local NLP
- Pattern matching
- Security checks

</td>
<td>

ğŸ” **Permission Guard**
- Security-focused
- Whitelist/blacklist
- Prevents abuse

</td>
</tr>
</table>

---

## ğŸ“ Project Structure

```
ğŸ€ AI Anime Assistant Backend
â”‚
â”œâ”€â”€ ğŸ™ï¸ Voice & Audio Processing
â”‚   â”œâ”€â”€ voice_chat.py              â­ Main orchestrator
â”‚   â”œâ”€â”€ vad_listener.py            ğŸ”Š Voice Activity Detection
â”‚   â””â”€â”€ audio_input.py             ğŸµ Audio capture
â”‚
â”œâ”€â”€ ğŸ§  AI & Language
â”‚   â”œâ”€â”€ llm_engine.py              ğŸ¤– Local LLM (Ollama)
â”‚   â”œâ”€â”€ emotion_engine.py          ğŸ˜Š Emotion detection
â”‚   â”œâ”€â”€ emotion_state.py           ğŸ’­ Emotion tracking
â”‚   â””â”€â”€ intent_parser.py           ğŸ” Intent recognition
â”‚
â”œâ”€â”€ ğŸ’¬ Response Generation
â”‚   â”œâ”€â”€ response_engine.py         ğŸ“ Response pipeline
â”‚   â”œâ”€â”€ voice_emotion_map.py       ğŸšï¸ Voice parameters
â”‚   â”œâ”€â”€ voice_clone.py             ğŸ¤ TTS synthesis
â”‚   â””â”€â”€ voice_clone_train.py       ğŸ‹ï¸ Model training
â”‚
â”œâ”€â”€ ğŸ’¾ Memory & Context
â”‚   â””â”€â”€ memory.py                  ğŸ“š Conversation history
â”‚
â”œâ”€â”€ ğŸ” Security & Actions
â”‚   â”œâ”€â”€ permission_guard.py        ğŸ›¡ï¸ Permission checks
â”‚   â”œâ”€â”€ action_controller.py       âš™ï¸ Action execution
â”‚   â””â”€â”€ system_actions.py          ğŸ–¥ï¸ System handlers
â”‚
â”œâ”€â”€ ğŸ“¦ Models & Data
â”‚   â”œâ”€â”€ tts/                       ğŸ—‚ï¸ Voice models
â”‚   â”œâ”€â”€ output/                    ğŸ“ Generated audio
â”‚   â””â”€â”€ logs/                      ğŸ“Š Debug logs
â”‚
â””â”€â”€ ğŸ§ª Testing
    â”œâ”€â”€ mic_test.py               ğŸ™ï¸ Audio testing
    â”œâ”€â”€ whisper_test.py           ğŸ—£ï¸ Speech testing
    â””â”€â”€ test_email.py             ğŸ“§ Email testing
```

---

## ğŸš€ Core Modules

### ğŸ™ï¸ Voice & Audio Processing

#### `voice_chat.py` â­
Main orchestrator combining voice and text input processing with emotional responses.

**ğŸ¯ Key Functions:**
- ğŸ™ï¸ `process_input(text)` - Handles both voice and text input
- ğŸ”Š Real-time VAD (Voice Activity Detection) listening
- ğŸ˜Š Emotion-aware response generation
- ğŸµ Voice synthesis with emotional parameters

#### `vad_listener.py` ğŸ”Š
Voice Activity Detection for capturing speech input when user is speaking.

#### `audio_input.py` ğŸµ
Raw audio input capture and preprocessing for voice analysis.

---

### ğŸ§  AI & Language Understanding

#### `llm_engine.py` ğŸ¤–
Integrates Ollama-based language models (Mistral, Phi3) for conversation.

**ğŸ”’ 100% Local & Offline:**
```
âœ… Ollama running on http://localhost:11434 (no internet required)
âœ… Model: Mistral (configurable, runs locally)
âœ… No API keys needed
âœ… No external service calls
âœ… All inference happens on your machine
âœ… Complete privacy - conversations never leave your computer
```

**ğŸ¯ Key Functions:**
- ğŸ¤– `generate_ai_reply(user_text, emotion, memory_context)` - Generate contextual responses locally

#### `emotion_engine.py` ğŸ˜Š
Detects and analyzes emotions using local transformer models.

**ğŸ”’ 100% Local & Offline:**
```
âœ… Uses DistilBERT emotion classifier (runs locally)
âœ… Downloaded once on first use (~500MB)
âœ… No API calls to cloud services
âœ… Processes emotions on your device
âœ… Fallback keyword detection for offline robustness
```

**ğŸ¯ Features:**
- ğŸ˜Š Maps emotions: joy, sadness, anger, fear, surprise, disgust
- ğŸ”„ Fallback detection for model unavailability
- ğŸ“Š Returns emotion scores and primary emotion

**ğŸ¯ Key Functions:**
- ğŸ˜Š `detect_emotion(text)` - Returns `(emotion, scores_dict)` - all local processing

#### `emotion_state.py` ğŸ’­
Manages current emotion state and state transitions.

#### `intent_parser.py` ğŸ”
Parses user intent for action triggering.

**ğŸ¯ Supported Intents:**
- ğŸŒ `open_browser` - Launch web browser
- ğŸ“ `open_notepad` - Open text editor
- ğŸš« `dangerous` - Blocked for security
- ğŸ’¬ `chat` - General conversation

---

### ğŸ’¬ Response Generation

#### `response_engine.py` ğŸ“
Orchestrates the full response generation pipeline combining emotion, intent, and memory.

#### `voice_emotion_map.py` ğŸšï¸
Maps emotional states to voice synthesis parameters (pitch, speed, etc.).

#### `voice_clone.py` ğŸ¤
Text-to-speech synthesis with custom voice cloning using XTTS model.

**ğŸ”’ 100% Local & Offline:**
```
âœ… XTTS model runs on your machine
âœ… No cloud TTS services (unlike Google, Azure, AWS TTS)
âœ… Voice synthesis happens locally
âœ… No audio is sent to external servers
âœ… Complete privacy for voice generation
```

**ğŸ¯ Key Functions:**
- ğŸ¤ `speak(text, emotion, voice_settings)` - Generate and play emotional speech locally

#### `voice_clone_train.py` ğŸ‹ï¸
Training pipeline for custom voice cloning models.

---

### ğŸ’¾ Memory & Context

#### `memory.py` ğŸ“š
Manages conversation history and contextual memory for coherent multi-turn conversations.

**ğŸ¯ Key Functions:**
- ğŸ“ Store and retrieve conversation context
- ğŸ“š Maintain user interaction history

---

### ğŸ” Security & Actions

#### `permission_guard.py` ğŸ›¡ï¸
Security verification system preventing unauthorized system actions.

**ğŸ¯ Features:**
- ğŸ” Permission checking for system actions
- âš¡ Safety enforcement
- ğŸ“‹ Whitelist/blacklist management

#### `action_controller.py` âš™ï¸
Executes permitted system actions based on user requests.

#### `system_actions.py` ğŸ–¥ï¸
Handlers for various system actions (open apps, control settings, etc.).

---

## ğŸ”§ Configuration

### âœ… Required Local Services

**ğŸ¤– Ollama LLM Service (Local - 100% Offline)**
```bash
ollama serve  # Start Ollama server on localhost:11434 (OFFLINE MODE)
```

**ğŸ“¦ Supported Models** (Downloaded once, cached locally):
- ğŸš€ `mistral` - Balanced performance
- âš¡ `phi3` - Faster responses (3.8B params)
- ğŸ’¬ `neural-chat` - Optimized for conversation
- ğŸ”§ Others supported by Ollama

> âœ… All models run 100% offline once downloaded to your local system.

---

### ğŸ“¥ Environment Setup

**1ï¸âƒ£ Install Local Dependencies** (No external API calls)
```bash
pip install requests          # For local Ollama communication
pip install transformers      # Local emotion detection models
pip install torch             # Local ML framework
pip install librosa           # Local audio processing
pip install soundfile         # Local audio I/O
pip install numpy             # Local numerical computing
```

**2ï¸âƒ£ Download Models (One-time setup)**

**ğŸ¤– Ollama Models** (Runs locally, ~4GB for Mistral):
```bash
ollama pull mistral      # Download to ~/.ollama/models/ (offline use)
ollama pull phi3         # Lightweight alternative (~2GB)
```

**ğŸ˜Š Emotion Detection Model** (Auto-downloads, ~500MB):
```
âœ… DistilBERT emotion classifier downloads on first run
âœ… Cached locally in ~/.cache/huggingface/
âœ… No API calls to Hugging Face servers
```

**ğŸ¤ Voice Synthesis Model** (XTTS):
```
âœ… Download XTTS model files to tts/ directory
âœ… Models are loaded on-demand by voice_clone.py
âœ… No streaming or external calls needed
```

**3ï¸âƒ£ Audio Setup** (Local)
```
âœ… Ensure microphone is configured on system
âœ… Test audio input with mic_test.py
```

---

## ğŸ“š Usage

### âœ… No API Keys Required

This project requires **zero external API keys** or subscriptions:

```
âœ… No OpenAI API          âœ… No commercial TTS services
âœ… No Google Cloud API    âœ… No cloud storage
âœ… No Azure API           âœ… No analytics or tracking
âœ… No AWS API             âœ… 100% FREE & OPEN
```

---

### ğŸ™ï¸ Basic Voice Chat (100% Offline)

```python
from voice_chat import process_input

# Process voice or text input
process_input("Hello, how are you?")
```

### ğŸ˜Š Emotion Detection

```python
from emotion_engine import detect_emotion

emotion, scores = detect_emotion("I'm so happy!")
print(f"Emotion: {emotion}, Scores: {scores}")
```

### ğŸ¤– Generate LLM Response

```python
from llm_engine import generate_ai_reply

response = generate_ai_reply(
    user_text="Tell me a joke",
    emotion="happy",
    memory_context="User likes programming"
)
print(response)
```

### ğŸ¤ Text-to-Speech

```python
from voice_clone import speak
from voice_emotion_map import get_voice_settings

voice_params = get_voice_settings("happy")
speak("That's wonderful!", emotion="happy", voice_settings=voice_params)
```

---

## ğŸ§ª Testing Utilities

| ğŸ”§ Tool | ğŸ“ Purpose |
|:-------:|:----------:|
| ğŸ™ï¸ `mic_test.py` | Microphone functionality testing |
| ğŸ—£ï¸ `whisper_test.py` | Speech recognition testing |
| ğŸ“§ `test_email.py` | Email integration testing |

---

##  Security Features

```
ğŸ”’ Permission Guards: Prevents unauthorized system access
âœ… Intent Validation: Validates user intent before execution
ğŸš« Action Blocking: Blocks shutdown/restart requests
âš¡ Safe Defaults: All actions are whitelisted, not blacklisted
```

---

```python
from voice_chat import process_input

# Process voice or text input
process_input("Hello, how are you?")
```

### Emotion Detection

```python
from emotion_engine import detect_emotion

emotion, scores = detect_emotion("I'm so happy!")
print(f"Emotion: {emotion}, Scores: {scores}")
```

### Generate LLM Response

```python
from llm_engine import generate_ai_reply

response = generate_ai_reply(
    user_text="Tell me a joke",
    emotion="happy",
    memory_context="User likes programming"
)
print(response)
```

### Text-to-Speech

```python
from voice_clone import speak
from voice_emotion_map import get_voice_settings

voice_params = get_voice_settings("happy")
speak("That's wonderful!", emotion="happy", voice_settings=voice_params)
```

## ğŸ§ª Testing Utilities

- `mic_test.py` - Microphone functionality testing
- `whisper_test.py` - Speech recognition testing
- `test_email.py` - Email integration testing

## ğŸ“Š Logging

Logs are stored in:
- `logs/action_controller_log.txt` - Action execution logs
- `output/` - Generated audio files and debug output

## ğŸ” Security Features

- **Permission Guards**: Prevents unauthorized system access
- **Intent Validation**: Validates user intent before execution
- **Dangerous Action Blocking**: Blocks shutdown/restart requests
- **Safe Defaults**: All actions are whitelisted, not blacklisted

## ğŸ”’ Offline & Privacy Features

### ğŸ¯ Zero External Calls

```
âœ… No API keys required          âœ… No cloud dependency
âœ… No data transmission           âœ… No tracking/telemetry  
âœ… No subscriptions              âœ… Complete privacy
âœ… No analytics                  âœ… Conversations stay local
```

### ğŸ“Š What Happens Offline

| ğŸ¯ Feature | ğŸ  Where It Runs | ğŸ”’ Data Privacy |
|:----------:|:----------------:|:----------------:|
| ğŸ§  **Language Model (LLM)** | Local Ollama server | Stays on device âœ… |
| ğŸ˜Š **Emotion Detection** | Local DistilBERT model | Stays on device âœ… |
| ğŸ¤ **Voice Synthesis** | Local XTTS model | Stays on device âœ… |
| ğŸ’¾ **Memory Storage** | Local file system | Stays on device âœ… |
| ğŸµ **Audio Processing** | Local PyAudio/Librosa | Stays on device âœ… |
| ğŸ” **Intent Detection** | Local regex/patterns | Stays on device âœ… |

### ğŸŒ Internet Connection

```
âŒ NOT required after setup
ğŸŒŸ Only needed for INITIAL model downloads
ğŸ” NO data is transmitted even if connected
ğŸ›¡ï¸ NO tracking or telemetry
```

### ğŸ¯ Perfect For

```
ğŸ”’ Privacy-conscious users           ğŸ“ Local development/testing
ğŸš« Restricted network environments   ğŸ›¡ï¸ Security-sensitive applications
ğŸ’» Offline machines                  ğŸ‘¨â€ğŸ’¼ Enterprise deployments
```

---

## ğŸ› Troubleshooting

### Ollama Connection Failed
- Ensure Ollama is running: `ollama serve`
- Check URL: `http://localhost:11434`
- Verify model availability: `ollama list`

### Emotion Detection Issues
- If DistilBERT model fails, system uses keyword-based fallback
- Models auto-download on first use (~500MB)

### Audio Issues
- Test microphone with `mic_test.py`
- Check system audio settings
- Verify PyAudio installation

### Voice Synthesis Issues
- Ensure XTTS model files exist in `tts/`
- Check audio output device configuration
- Review `output/` directory for generated files

## ğŸ¨ Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ğŸ€ AI ANIME ASSISTANT                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

User Input (Voice ğŸ™ï¸ / Text ğŸ’¬)
         â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ VAD Listenerâ”‚  â† ğŸ§ Voice Activity Detection
    â”‚ Text Parser â”‚  â† ğŸ“ Parse User Input
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Emotion Engine  â”‚  â† ğŸ˜Š Detect emotion
    â”‚ (DistilBERT)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Intent Parser   â”‚  â† ğŸ” What does user want?
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Permission Guardâ”‚  â† ğŸ›¡ï¸ Is action allowed?
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ LLM Engine      â”‚  â† ğŸ¤– Generate response
    â”‚ (Ollama Local)  â”‚    (Mistral/Phi3)
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Response Engine â”‚  â† ğŸ“ Build response
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Voice Emotion Map    â”‚  â† ğŸšï¸ Set voice params
    â”‚ (Pitch/Speed/Tone)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Voice Synthesis  â”‚  â† ğŸ¤ Generate audio
    â”‚ (XTTS Local)     â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ğŸ”Š Audio Output          â”‚
    â”‚ ğŸ’¾ Memory Storage        â”‚
    â”‚ ğŸ“Š Action Logging        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  âœ… Everything stays local â€¢ ğŸ”’ 100% Privacy â€¢ âš¡ Offline  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš¦ State Management

**ğŸ˜Š Emotion State:**
```
â€¢ Current emotion: happy, sad, angry, calm, fear, surprise
â€¢ Intensity: 0.0 - 1.0 (scale)
â€¢ Auto-updates: Based on user input
```

**ğŸ’¾ Memory State:**
```
â€¢ Conversation history (stored locally)
â€¢ User preferences
â€¢ Context windows for multi-turn dialogue
```

---

## ğŸ“¦ Dependencies

### âœ… Core (All Local Processing)

| ğŸ“¦ Package | ğŸ¯ Purpose |
|:----------:|:----------:|
| `requests` | HTTP requests to **local** Ollama (not external services) |
| `transformers` | Local emotion classification models |
| `torch` | Local deep learning framework |
| `librosa` | Local audio processing |
| `soundfile` | Local audio I/O |
| `numpy` | Local numerical computing |

### ğŸ”§ Optional

- `ollama-python` - Alternative local Ollama client
- `pyttsx3` - Local text-to-speech fallback
- `pyaudio` - Local audio device management

### âœ… Zero External Dependencies
```
âŒ No cloud SDKs (AWS, Azure, Google Cloud)
âŒ No API client libraries
âŒ No telemetry/tracking libraries
âŒ No subscription-based services
```

---

## ğŸ“ License

This project is part of the AI Anime Assistant suite.

---

## ğŸ¤ Contributing

Contributions welcome! Please ensure:
- âœ… Code follows existing style conventions
- âœ… Emotion detection maintains accuracy
- âœ… Voice output quality is tested
- âœ… Security features remain intact
- âœ… Documentation is updated with changes

---

## ğŸ› Troubleshooting

### âŒ Ollama Connection Failed
```bash
# Solution:
ollama serve                    # Ensure Ollama is running
# Check URL: http://localhost:11434
ollama list                    # Verify model availability
```

### âŒ Emotion Detection Issues
```
âœ… If DistilBERT model fails, system uses keyword-based fallback
âœ… Models auto-download on first use (~500MB)
```

### âŒ Audio Issues
```bash
# Solutions:
python mic_test.py            # Test microphone
# Check system audio settings
# Verify PyAudio installation
```

### âŒ Voice Synthesis Issues
```
âœ… Ensure XTTS model files exist in tts/
âœ… Check audio output device configuration
âœ… Review output/ directory for generated files
```

---

## ğŸ“ Support & Resources

For issues or questions:

| ğŸ”§ Resource | ğŸ“ Details |
|:----------:|:----------:|
| ğŸ“Š **Logs** | Check `logs/` directory for debugging |
| ğŸ§ª **Test Utilities** | Run provided test scripts |
| ğŸ” **Model Check** | Verify Ollama & model availability |
| ğŸ“– **Documentation** | Review this README for detailed info |

---

<div align="center">

## ğŸŒŸ Status & Info

[![Status](https://img.shields.io/badge/Status-Active%20Development-brightgreen?style=flat-square)](https://github.com)
[![Last Updated](https://img.shields.io/badge/Last%20Updated-January%202026-blue?style=flat-square)](https://github.com)
[![Python](https://img.shields.io/badge/Python-3.8%2B-informational?style=flat-square)](https://www.python.org)
[![License](https://img.shields.io/badge/License-MIT-yellow?style=flat-square)](https://github.com)

### ğŸ¯ Key Stats

```
âœ… 100% Offline        ğŸ”’ Privacy-First       âš¡ No APIs
ğŸ’» Local Processing    ğŸµ Voice Enabled       ğŸ¤– AI-Powered
ğŸ›¡ï¸ Secure            ğŸ“± Multi-Device        ğŸš€ Production Ready
```

---

**Made with â¤ï¸ for privacy-conscious users**  
**AI Anime Assistant Backend v1.0**

</div>
